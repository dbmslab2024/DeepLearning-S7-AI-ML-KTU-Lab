# Recurrent Neural Networks (RNN) for Sentiment Analysis

## Theory
Recurrent Neural Networks (RNNs) are specialized neural networks designed to work with sequential data by maintaining a memory of previous inputs. Unlike feedforward networks, RNNs have connections that form cycles, allowing information to persist through time steps. This architecture makes them particularly effective for natural language processing tasks like sentiment analysis, where context and word order matter significantly.

In sentiment analysis, an RNN processes text sequentially, word by word, maintaining a hidden state that captures information about the sequence seen so far. This hidden state is updated at each time step based on the current input word and the previous hidden state. For sentiment classification, the final hidden state (or sometimes an aggregation of all hidden states) is fed into a classification layer to predict whether the sentiment is positive or negative.

Traditional RNNs suffer from the vanishing gradient problem when processing long sequences. More advanced variants like Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRU) were developed to address this limitation by incorporating gating mechanisms that allow the network to selectively remember or forget information.

## 10-Step Algorithm for RNN-based IMDB Review Classification

1. Load and preprocess the IMDB dataset (tokenize reviews, create vocabulary)
2. Convert text data to numerical sequences and perform padding/truncation
3. Split data into training and validation sets
4. Define the RNN architecture (embedding layer, RNN/LSTM/GRU layer, output layer)
5. Initialize model parameters and optimization criteria (loss function, optimizer)
6. Implement training loop to iterate over batches of training data
7. Forward pass: process input sequences through the RNN model
8. Calculate loss between predictions and actual sentiment labels
9. Backward pass: update model weights using gradient descent
10. Evaluate model performance on validation set and test with sample predictions

## PyTorch Implementation

Here's a complete implementation of an RNN for IMDB sentiment analysis:

```python name=rnn_sentiment_analysis.py
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
from torchtext.datasets import IMDB
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
from torch.utils.data import DataLoader
from torch.nn.utils.rnn import pad_sequence

# Set random seed for reproducibility
torch.manual_seed(42)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 1. Load and preprocess the IMDB dataset
tokenizer = get_tokenizer('basic_english')

def yield_tokens(data_iter):
    for _, text in data_iter:
        yield tokenizer(text)

# Load IMDB dataset
train_iter = IMDB(split='train')
test_iter = IMDB(split='test')
train_iter_for_vocab = IMDB(split='train')

# Build vocabulary
vocab = build_vocab_from_iterator(yield_tokens(train_iter_for_vocab), 
                                  min_freq=5, 
                                  specials=['<unk>', '<pad>'])
vocab.set_default_index(vocab['<unk>'])

text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]
label_pipeline = lambda x: 1 if x == 'pos' else 0

# 2. Data processing functions
def collate_batch(batch):
    label_list, text_list = [], []
    for label, text in batch:
        label_list.append(label_pipeline(label))
        processed_text = torch.tensor(text_pipeline(text), dtype=torch.int64)
        text_list.append(processed_text)
    
    # Pad sequences to make them the same length
    padded_text = pad_sequence(text_list, batch_first=True, padding_value=vocab['<pad>'])
    
    # Truncate sequences that are too long (optional)
    max_length = 500
    if padded_text.shape[1] > max_length:
        padded_text = padded_text[:, :max_length]
        
    return torch.tensor(label_list, dtype=torch.float32).to(device), padded_text.to(device)

# 3. Create data loaders
batch_size = 64
train_dataloader = DataLoader(list(train_iter), batch_size=batch_size, 
                             shuffle=True, collate_fn=collate_batch)
test_dataloader = DataLoader(list(test_iter), batch_size=batch_size, 
                            shuffle=False, collate_fn=collate_batch)

# 4. Define RNN model
class RNNModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # You can use RNN, LSTM or GRU here
        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, text):
        # text shape: [batch_size, seq_len]
        embedded = self.embedding(text)
        # embedded shape: [batch_size, seq_len, embedding_dim]
        
        output, (hidden, cell) = self.rnn(embedded)
        # hidden shape: [1, batch_size, hidden_dim]
        
        # We'll use the last hidden state for classification
        hidden = hidden.squeeze(0)
        # hidden shape: [batch_size, hidden_dim]
        
        return self.fc(hidden)

# 5. Initialize model
vocab_size = len(vocab)
embedding_dim = 100
hidden_dim = 128
output_dim = 1  # Binary classification

model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.BCEWithLogitsLoss()

# 6. Training function
def train(model, dataloader, optimizer, criterion):
    model.train()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0
    
    for labels, text in dataloader:
        optimizer.zero_grad()
        
        # 7. Forward pass
        predictions = model(text).squeeze(1)
        
        # 8. Calculate loss
        loss = criterion(predictions, labels)
        
        # 9. Backward pass and update weights
        loss.backward()
        optimizer.step()
        
        total_loss += loss.item()
        
        # Calculate accuracy
        rounded_preds = torch.round(torch.sigmoid(predictions))
        correct_predictions += (rounded_preds == labels).sum().item()
        total_predictions += labels.size(0)
        
    return total_loss / len(dataloader), correct_predictions / total_predictions

# Evaluation function
def evaluate(model, dataloader, criterion):
    model.eval()
    total_loss = 0
    correct_predictions = 0
    total_predictions = 0
    
    with torch.no_grad():
        for labels, text in dataloader:
            predictions = model(text).squeeze(1)
            loss = criterion(predictions, labels)
            
            total_loss += loss.item()
            
            rounded_preds = torch.round(torch.sigmoid(predictions))
            correct_predictions += (rounded_preds == labels).sum().item()
            total_predictions += labels.size(0)
            
    return total_loss / len(dataloader), correct_predictions / total_predictions

# 10. Training loop
n_epochs = 5
for epoch in range(n_epochs):
    train_loss, train_acc = train(model, train_dataloader, optimizer, criterion)
    test_loss, test_acc = evaluate(model, test_dataloader, criterion)
    
    print(f'Epoch: {epoch+1}')
    print(f'Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.3f}')
    print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.3f}')
    print('-' * 30)

# Function to predict sentiment of new reviews
def predict_sentiment(model, text, tokenizer=tokenizer, vocab=vocab, device=device):
    model.eval()
    tokens = tokenizer(text)
    indexed = [vocab[t] for t in tokens]
    tensor = torch.tensor(indexed).unsqueeze(0).to(device)
    
    with torch.no_grad():
        prediction = torch.sigmoid(model(tensor))
        
    sentiment = "Positive" if prediction.item() > 0.5 else "Negative"
    confidence = prediction.item() if sentiment == "Positive" else 1 - prediction.item()
    
    return sentiment, confidence

# Example predictions
sample_reviews = [
    "This movie was fantastic! I really loved it and would definitely recommend it.",
    "The worst film I've seen in my life. Absolutely terrible acting and plot.",
    "The movie was okay, not great but not terrible either.",
    "I was pleasantly surprised by how good this film turned out to be."
]

print("\nSample Predictions:")
for review in sample_reviews:
    sentiment, confidence = predict_sentiment(model, review)
    print(f"\nReview: {review}")
    print(f"Sentiment: {sentiment}")
    print(f"Confidence: {confidence:.4f}")
```

This implementation includes the complete workflow for training an RNN-based sentiment classifier on the IMDB dataset and making predictions on new reviews. The model uses an LSTM (a type of RNN) with an embedding layer to convert words to vectors and a fully connected layer for the final classification.

The code demonstrates how to:
1. Load and preprocess the IMDB dataset
2. Build a vocabulary from the dataset
3. Create data loaders with appropriate padding
4. Define an LSTM-based model architecture
5. Train the model with appropriate loss function and optimizer
6. Evaluate model performance
7. Make predictions on new reviews

When you run this code, you'll see the training progress and final sample predictions with confidence scores, showing how the model classifies different types of movie reviews.