[3/07/2025 4:10 PM] Abhinav AI MACE: Please find the programs to be completed tomorrow. Each student is required to bring a rough record with the algorithm written in it.
[3/07/2025 4:10 PM] Abhinav AI MACE: 1.Write a program to demonstrate the working of the decision tree based ID3 algorithm.
                                     Use play tennis data set for building the decision tree and apply this knowledge to
                                     classify a new sample.
[3/07/2025 4:10 PM] Abhinav AI MACE: 2. To implement and evaluate a linear Support Vector Machine (SVM) classifier for the Iris dataset.
[3/07/2025 4:10 PM] Abhinav AI MACE: 3. To implement and visualize the Gradient Descent algorithm to find the optimal parameters (slope and intercept) for a simple linear regression model using an apporpriate data set.
[3/07/2025 4:10 PM] Abhinav AI MACE: Kindly convey this message to the entire class and ensure students follow the instructions according to their respective lab slots.
[3/07/2025 4:10 PM] Abhinav AI MACE: Tomorrow DL LAB for Batch 1
-----------------------------------------------
[3/07/2025 4:10 PM] Abhinav AI MACE: ðŸ›‘ðŸ›‘ðŸ›‘
[3/07/2025 8:37 PM] Abhinav AI MACE: Don't forget
[17/07/2025 3:47 PM] Abhinav AI MACE: 
LAB QUESTIONS FOR TOMOROW: 
                                    1. Implement Simple Linear Regression with Synthetic Data.      
                                    2. Implement basic image enhancement operations such as histogram equalization, morphological operations.
[17/07/2025 3:47 PM] Abhinav AI MACE: The program has to done in vs code using pytorch
[27/07/2025 8:22 PM] Abhinav AI MACE: ðŸ›‘
[27/07/2025 8:23 PM] Abhinav AI MACE: Naale lab is for batch 2
----------------------------------------------------
[3/08/2025 5:41 PM] Abhinav AI MACE: 
Implement Feed forward neural network with three hidden layers for classification on CIFAR-10 dataset. Design and train a neural network that achieves high accuracy in classifying the images into their respective classes. Test for different hyper-parameters-  Run 1:
Hidden units: (512, 256, 128)
Activation: relu
- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 2:
Hidden units: (512, 256, 128)
Activation: tanh

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 3:
Hidden units: (512, 256, 128)
Activation: sigmoid

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 4:
Hidden units: (256, 128, 64)
Activation: relu

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 5:
Hidden units: (256, 128, 64)
Activation: tanh

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 6:
Hidden units: (256, 128, 64)
Activation: sigmoid

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 7:
Hidden units: (1024, 512, 256)
Activation: relu

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 8:
Hidden units: (1024, 512, 256)
Activation: tanh

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 9:
Hidden units: (1024, 512, 256)
Activation: sigmoid
[3/08/2025 5:42 PM] Abhinav AI MACE: 
Implement a feed forward neural network with three hidden 
                layers for the CIFAR-10 dataset. Train the network using a baseline optimization 
                algorithm, such as Adam, without any specific weight initialization 
                technique or regularization technique. Record the accuracy and loss
                 during training. (a) Repeat the training process with Xavier initialization 
                 for weight initialization. Compare the convergence speed and accuracy of the 
                 network with the baseline results. Analyze the impact of Xavier initialization 
                 on the network's performance. (b) Repeat the training process with Kaiming
                  initialization for weight initialization. Compare the convergence speed and
                   accuracy of the network with the baseline results. Analyze the impact of
                    Kaiming initialization on the network's performance. (c) Implement dropout 
                    regularization by applying dropout to the hidden layers of the network. 
                    Train the network with dropout regularization and compare its performance 
                    with the baseline results. Analyze the impact of dropout on the network's 
                    performance in terms of accuracy and overfitting (d) Implement L2 
                    regularization techniques by adding a regularization term to the loss
                     function during training. Train the network with regularization and 
                     compare its performance with the baseline results. Analyze the impact
                      of regularization on the network's performance in terms of accuracy and
                       prevention of overfitting.
[3/08/2025 5:42 PM] Abhinav AI MACE: Deep Learning lab program questions for tomorrow.
[3/08/2025 5:45 PM] Abhinav AI MACE: Everyone in tomorrow's batch, should submit their 
                    rough records completed up to the third experiment 
                    along with the given program algorithm in tomorrow's lab hour.
---------------------------------------
[24/08/2025 3:14 PM] Abhinav AI MACE: 
Exp 6- Implement a Convolutional Neural Network (CNN) 
                                             architecture for digit classification on the MNIST dataset. 
                                             Design and train a CNN model that achieves high accuracy in 
                                             recognizing handwritten digits.
[24/08/2025 3:14 PM] Abhinav AI MACE:
Exp 7 - Digit classification using pre-trained 
                                              networks like VGGnet-19 for MNIST dataset and 
                                              analyse and visualize performance improvement.
                                              Explore transfer learning using Convolutional 
                                              Neural Networks (ConvNets) as fixed feature 
                                              extractors and fine-tuning for image classification. 
                                              Analyze their performance on a new image classification
                                              task while comparing the fixed feature extractor approach with fine-tuning.
[24/08/2025 3:20 PM] Abhinav AI MACE: Lab for BATCH 2
-------------------------------------------

[2:25 pm, 18/09/2025] Abhinav AI MACE: 
expt 8: Implement a Recurrent Neural Network (RNN) 
      for review classification on the IMDB dataset. Design and train an RNN model to classify 
      movie reviews as positive or negative based on their sentiment.
[2:25 pm, 18/09/2025] Abhinav AI MACE: 
exp 9: Analyze and visualize the performance change 
      while using LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) instead of a standard
      RNN (Recurrent Neural Network) for sentiment analysis on the IMDB dataset. Compare the 
      performance of different RNN architectures and understand their impact on sentiment classification.
-------------------------------------------
[3:55 pm, 28/09/2025] Abhinav AI MACE: Batch 2
[3:55 pm, 28/09/2025] Abhinav AI MACE: It will be their last lab for deep learning
[3:55 pm, 28/09/2025] Abhinav AI MACE: 
Exp 10 Implement time series forecasting for the 
      NIFTY-50 dataset. Design and train a model to predict future values of the NIFTY-50 
      stock market index based on historical data
Exp 11 Implement a shallow autoencoder and decoder network for machine translation using 
      the Kaggle English to Hindi Neural Translation Dataset. Design and train a model to
      translate English sentences to Hindi by leveraging the power of autoencoders and de coders
