[3/07/2025 4:10 PM] Abhinav AI MACE: Please find the programs to be completed tomorrow. Each student is required to bring a rough record with the algorithm written in it.
[3/07/2025 4:10 PM] Abhinav AI MACE: 1.Write a program to demonstrate the working of the decision tree based ID3 algorithm.
                                     Use play tennis data set for building the decision tree and apply this knowledge to
                                     classify a new sample.
[3/07/2025 4:10 PM] Abhinav AI MACE: 2. To implement and evaluate a linear Support Vector Machine (SVM) classifier for the Iris dataset.
[3/07/2025 4:10 PM] Abhinav AI MACE: 3. To implement and visualize the Gradient Descent algorithm to find the optimal parameters (slope and intercept) for a simple linear regression model using an apporpriate data set.
[3/07/2025 4:10 PM] Abhinav AI MACE: Kindly convey this message to the entire class and ensure students follow the instructions according to their respective lab slots.
[3/07/2025 4:10 PM] Abhinav AI MACE: Tomorrow DL LAB for Batch 1
------------------------------------
[3/07/2025 4:10 PM] Abhinav AI MACE: ðŸ›‘ðŸ›‘ðŸ›‘
[3/07/2025 8:37 PM] Abhinav AI MACE: Don't forget
[17/07/2025 3:47 PM] Abhinav AI MACE: LAB QUESTIONS FOR TOMOROW: 
                                    1. Implement Simple Linear Regression with Synthetic Data.      
                                    2. Implement basic image enhancement operations such as histogram equalization, morphological operations.
[17/07/2025 3:47 PM] Abhinav AI MACE: The program has to done in vs code using pytorch
[27/07/2025 8:22 PM] Abhinav AI MACE: ðŸ›‘
[27/07/2025 8:23 PM] Abhinav AI MACE: Naale lab is for batch 2
------------------------------------
[3/08/2025 5:41 PM] Abhinav AI MACE: 
Implement Feed forward neural network with three hidden layers for classification on CIFAR-10 dataset. Design and train a neural network that achieves high accuracy in classifying the images into their respective classes. Test for different hyper-parameters-  Run 1:
Hidden units: (512, 256, 128)
Activation: relu
- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 2:
Hidden units: (512, 256, 128)
Activation: tanh

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 3:
Hidden units: (512, 256, 128)
Activation: sigmoid

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 4:
Hidden units: (256, 128, 64)
Activation: relu

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 5:
Hidden units: (256, 128, 64)
Activation: tanh

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 6:
Hidden units: (256, 128, 64)
Activation: sigmoid

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 7:
Hidden units: (1024, 512, 256)
Activation: relu

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 8:
Hidden units: (1024, 512, 256)
Activation: tanh

- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -
Run 9:
Hidden units: (1024, 512, 256)
Activation: sigmoid
[3/08/2025 5:42 PM] Abhinav AI MACE: Implement a feed forward neural network with three hidden 
                layers for the CIFAR-10 dataset. Train the network using a baseline optimization 
                algorithm, such as Adam, without any specific weight initialization 
                technique or regularization technique. Record the accuracy and loss
                 during training. (a) Repeat the training process with Xavier initialization 
                 for weight initialization. Compare the convergence speed and accuracy of the 
                 network with the baseline results. Analyze the impact of Xavier initialization 
                 on the network's performance. (b) Repeat the training process with Kaiming
                  initialization for weight initialization. Compare the convergence speed and
                   accuracy of the network with the baseline results. Analyze the impact of
                    Kaiming initialization on the network's performance. (c) Implement dropout 
                    regularization by applying dropout to the hidden layers of the network. 
                    Train the network with dropout regularization and compare its performance 
                    with the baseline results. Analyze the impact of dropout on the network's 
                    performance in terms of accuracy and overfitting (d) Implement L2 
                    regularization techniques by adding a regularization term to the loss
                     function during training. Train the network with regularization and 
                     compare its performance with the baseline results. Analyze the impact
                      of regularization on the network's performance in terms of accuracy and
                       prevention of overfitting.
[3/08/2025 5:42 PM] Abhinav AI MACE: Deep Learning lab program questions for tomorrow.
[3/08/2025 5:45 PM] Abhinav AI MACE: Everyone in tomorrow's batch, should submit their 
                    rough records completed up to the third experiment 
                    along with the given program algorithm in tomorrow's lab hour.